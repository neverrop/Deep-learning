asasaafdas
% !TeX encoding=utf8
% !TeX spellcheck = en-US

\chapter{related works}

In this part, we give a brief introduction for some important works that related to our research, consists of learning storyline, video summarization and gaze in egocentric videos.

\section{Learning storyline}

The original form of storyline can be traced back to the 1970-80s, where $scripts$~\cite{Schank1988SCRIPTS} (structured representations of causation relationships, events, participants, $etc.$) are used as knowledge backbones for tasks like text summarization and question answering. Then in~\cite{Ivanov2000Recognition}, Yuri and Bobick proposed a method which combines statistical techniques used to detect primitive components of an activity with syntactic recognition of the process' structure. However, these methods either based on scripts (structured representations of events, causation relationships, participants, $etc.$) or stochastic grammar, both require extra human efforts or annotation of videos, which fundamentally limits their usage in an open domain. This motivates the recent developments of novel approaches that can learn underlying visual storylines automatically from images or videos. In~\cite{Kim2014Reconstructing}, Gunhee and Eric proposed an automatic storyline-learning method, which formulated as an inference problem of sparse time-varying directed graphs (\eg~\cite{Song2009Time}). Recently, since there have been momentous successes in using CNN~\cite{Krizhevsky2012ImageNet} features along with Recurrent Neural Networks~\cite{Yang2015Stacked,Zhu2015Visual7W} (RNNs) to represent those temporal dynamics in data~\cite{Karpathy2017Deep,Donahue2015Long}. So Gunnar \etal~\cite{srnn} extended that idea to modeling the dynamics in storylines by introducing Skipping Recurrent Neural Network model, based and refined on RNN.

\section{Video Summarization}

The problem of video summarization has been wildly studied from various perspectives\cite{Ngo2003AutomaticVS,Zhang2017Deep}. Most methods select a sequence of keyframes or subshots to form a visual summary of the most informative or interesting parts of a video. Many previous approaches have been designed to seek cues from low-level motion and appearances \cite{DeMenthon1998VideoSB,Ngo2005VideoSA}. Some recent approaches extract scenes of interest by training a supervised model of important objects \cite{Khosla2013LargeScaleVS,Liu2010AHV}, multiview \cite{Fu2010MultiViewVS} and user interactions \cite{Ellouze2010IMS2IM}. Then more external factors are taken into consideration for summarization besides the narrative structure. For instance, in \cite{Sinha2011SummarizationOP} the authors put forward three criteria: {}quality, diversity, and coverage. And later the social context (e.g. characters and aesthetics) is also considered into the summarization framework in \cite{Obrador2010SupportingPP}. The work in \cite{Lu2013Story} is most similar with our job, which is also trying to learn a underlying story for egocentric videos. In their work, they summary scene of story by selected short chain of video subshots, and then capture the event connectivity beyond simple object co-occurrence. But first they need to split the video based on the camera movement, which, sometimes cannot seperate the action correctly. Besides, as mentioned in their experiment, they does not have much advantage when the story is uneventful, or when there are multiple interwoven threads (e.g., cooking soup and making cookies at the same time). In such cases, their method tends to select a chain of subshots that are influential to each other, but miss other important parts of the story. In a sense, such multi-tasking is inherently breaking the visual storyline. In conclusion, most of these methods are either supervised, which means the associated summaries for videos are first collected by crowd-sourcing, then a model is learned to generate good summaries, or constrained to a certain domain. Considering the task of summarization will be less ambiguous when the concept is given, we'd like to explore how to summarize the video in a both intelligent and efficient way.

\section{Gaze in egocentric videos}

In \cite{Judd2009Learning}, they use low-level features to learn a model of saliency directly from human eye movement data. Ba \et \cite{Ba2011MultipersonVF} proposed to analyze human visual attention by exploring correlations between head orientation and gaze direction. Similarly, Yamada \et \cite{Yamada2011Attention} presented gaze prediction models and explored the correlation between gaze and head motion with the aid of external motion sensors. However, these sensors may increase the loads and power consumption of wearable devices. Later, Yin \et \cite{Li2014Learning} proposed the model on gaze prediction in hand-object manipulation tasks, which combines hand detection and pose recognition to provide primary egocentric cues in their model. However, their model may not generalize well to various egocentric activities especially when hands are not involved, due to the egocentric cues are predefined. Most recently, Zhang \et \cite{Zhang2017Deep} proposed a novel GAN-based model which can learn essential egocentric cues automatically during the training phase. Considering the moving background of egocentric videos, caused by the head motion, they adapt the two-stream video model \cite{Vondrick2016GeneratingVW} with both streams replaced by 3D-CNN to explicitly untangle background and foreground motions.



